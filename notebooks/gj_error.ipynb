{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import process\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import http.client\n",
    "import json\n",
    "import urllib.parse\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "mongodb_conn = os.getenv('MONGODB_CONNECTION_STRING')\n",
    "\n",
    "# Global variables to keep track of searched job titles and cities\n",
    "searched_jobs = set()\n",
    "searched_cities = set()\n",
    "\n",
    "def google_job_search(job_title, city_state, start=0):\n",
    "    '''\n",
    "    job_title(str): \"Data Scientist\", \"Data Analyst\"\n",
    "    city_state(str): \"Denver, CO\"\n",
    "    '''\n",
    "    query = f\"{job_title} {city_state}\"\n",
    "    params = {\n",
    "        \"api_key\": os.getenv('WEBSCRAPING_API_KEY'),\n",
    "        \"engine\": \"google_jobs\",\n",
    "        \"q\": query,\n",
    "        \"hl\": \"en\",\n",
    "        # \"google_domain\": \"google.com\",\n",
    "        # \"start\": start,\n",
    "        # \"chips\": f\"date_posted:{post_age}\",\n",
    "    }\n",
    "\n",
    "    query_string = urllib.parse.urlencode(params, quote_via=urllib.parse.quote)\n",
    "\n",
    "    conn = http.client.HTTPSConnection(\"serpapi.webscrapingapi.com\")\n",
    "    try:\n",
    "        conn.request(\"GET\", f\"/v1?{query_string}\")\n",
    "        print(f\"GET /v1?{query_string}\")\n",
    "        res = conn.getresponse()\n",
    "        try:\n",
    "            data = res.read()\n",
    "        finally:\n",
    "            res.close()\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "    try:\n",
    "        json_data = json.loads(data.decode(\"utf-8\"))\n",
    "        jobs_results = json_data['google_jobs_results']\n",
    "        return jobs_results\n",
    "    except (KeyError, json.JSONDecodeError) as e:\n",
    "        print(f\"Error occurred for search: {job_title} in {city_state}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        print(f\"Data: {data}\")\n",
    "        return None\n",
    "\n",
    "def mongo_dump(jobs_results, collection_name):\n",
    "    client = MongoClient(mongodb_conn)\n",
    "    db = client.job_search_db\n",
    "    collection = db[collection_name]\n",
    "    \n",
    "    for job in jobs_results:\n",
    "        job['retrieve_date'] = dt.datetime.today().strftime('%Y-%m-%d')\n",
    "        collection.insert_one(job)\n",
    "    \n",
    "    print(f\"Dumped {len(jobs_results)} documents to MongoDB collection {collection_name}\")\n",
    "\n",
    "def process_batch(job, city_state, start=0):\n",
    "    global searched_jobs, searched_cities\n",
    "\n",
    "    # Check if the job title and city have already been searched\n",
    "    if (job, city_state) in searched_jobs:\n",
    "        print(f'Skipping already searched job: {job} in {city_state}')\n",
    "        return\n",
    "\n",
    "    jobs_results = google_job_search(job, city_state, start)\n",
    "    if jobs_results is not None:\n",
    "        print(f'City: {city_state} Job: {job} Start: {start}')\n",
    "        mongo_dump(jobs_results, 'sf_bay_test_jobs')\n",
    "\n",
    "        # Add the job title and city to the searched sets\n",
    "        searched_jobs.add((job, city_state))\n",
    "        searched_cities.add(city_state)\n",
    "\n",
    "def main(job_list, city_state_list):\n",
    "    for job in job_list:\n",
    "        for city_state in city_state_list:\n",
    "            output = process_batch(job, city_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_list = [\"Data Scientist\", \"Machine Learning Engineer\", \"AI Gen Engineer\", \"ML Ops\"]\n",
    "city_state_list = [\"Atlanta, GA\", \"Austin, TX\", \"Boston, MA\", \"Chicago, IL\", \n",
    "                \"Denver CO\", \"Dallas-Ft. Worth, TX\", \"Los Angeles, CA\",\n",
    "                \"New York City NY\", \"San Francisco, CA\", \"Seattle, WA\",\n",
    "                \"Palo Alto CA\", \"Mountain View CA\", \"San Jose, CA\"]\n",
    "simple_city_state_list: list[str] = [\"Palo Alto CA\", \"San Francisco CA\", \"Mountain View CA\"]\n",
    "main(job_list, simple_city_state_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping already searched job: Data Scientist in San Francisco, CA\n"
     ]
    }
   ],
   "source": [
    "process_batch(\"Data Scientist\", \"San Francisco, CA\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(mongodb_conn)\n",
    "db = client.job_search_db\n",
    "collection = db['sf_bay_test_jobs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datajobs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
